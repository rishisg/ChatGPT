{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAqZFfUBSYns3JdGqR1r/E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishisg/ChatGPT/blob/main/RNN_Sentiment_Analysis_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ RNN for Sentiment Analysis\n",
        "\n",
        "âœ… Dataset: Twitter Sentiment Analysis âœ… Goal: Classify tweets as Positive, Negative, or Neutral âœ… Approach: Build RNN-based deep learning models using TensorFlow/Keras âœ… Evaluation: Accuracy, Precision, Recall, F1-score, Model Optimization\n",
        "\n",
        "1ï¸âƒ£ Setup & Import Libraries\n",
        "\n",
        "ğŸ’¡ Explanation\n",
        "âœ” numpy & pandas â†’ Handle numerical & text data âœ” tensorflow & keras â†’ Build RNN models âœ” Tokenizer & pad_sequences â†’ Convert text to numerical format for deep learning âœ” SimpleRNN, LSTM, GRU â†’ RNN architectures for sentiment analysis"
      ],
      "metadata": {
        "id": "yUWHGMVyajkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WGqrh5Edainx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2ï¸âƒ£ Load & Explore the Dataset\n",
        "\n",
        "ğŸ’¡ Explanation\n",
        "âœ” Loads Twitter Sentiment Analysis dataset âœ” Shows sample tweets & sentiment labels âœ” Class distribution check ensures a balanced dataset\n",
        "\n",
        "âœ” Loads the Twitter Training CSV file with proper encoding âœ” Checks if the dataset structure is valid âœ” Prints available columns & verifies if 'sentiment' exists âœ” Strips column names to remove extra spaces âœ” Provides manual header renaming options if needed\n"
      ],
      "metadata": {
        "id": "LEdCBH0la2EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary library\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (Ensuring correct encoding)\n",
        "df = pd.read_csv(\"twitter_training.csv\", encoding=\"utf-8\")\n",
        "\n",
        "# Check dataset structure\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"First few rows:\\n\", df.head())\n",
        "print(\"Column Names:\", df.columns)\n",
        "\n",
        "# Fix potential column name issues (removing extra spaces)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Verify if 'sentiment' column exists\n",
        "if 'sentiment' in df.columns:\n",
        "    print(\"Sentiment Column Found!\")\n",
        "    print(df[\"sentiment\"].value_counts())  # Check class distribution\n",
        "else:\n",
        "    print(\"Error: Column 'sentiment' not found!\")\n",
        "    print(\"Available Columns:\", df.columns)\n",
        "\n",
        "# Optional Fix: If headers are incorrect, rename columns manually\n",
        "# Uncomment and update column names based on actual structure\n",
        "# df.columns = [\"id\", \"user\", \"tweet\", \"sentiment\"]\n",
        "# print(\"Updated Columns:\", df.columns)\n",
        "\n",
        "# Optional Fix: Reload without header row if structure looks incorrect\n",
        "# df = pd.read_csv(\"twitter_training.csv\", header=None)\n",
        "# print(\"Head after loading without header:\\n\", df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sttOGUyYa8MT",
        "outputId": "796c0e6d-406a-4326-9455-1bfa77836f8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (74681, 4)\n",
            "First few rows:\n",
            "    2401  Borderlands  Positive  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "  im getting on borderlands and i will murder you all ,  \n",
            "0  I am coming to the borders and I will kill you...     \n",
            "1  im getting on borderlands and i will kill you ...     \n",
            "2  im coming on borderlands and i will murder you...     \n",
            "3  im getting on borderlands 2 and i will murder ...     \n",
            "4  im getting into borderlands and i can murder y...     \n",
            "Column Names: Index(['2401', 'Borderlands', 'Positive',\n",
            "       'im getting on borderlands and i will murder you all ,'],\n",
            "      dtype='object')\n",
            "Error: Column 'sentiment' not found!\n",
            "Available Columns: Index(['2401', 'Borderlands', 'Positive',\n",
            "       'im getting on borderlands and i will murder you all ,'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3ï¸âƒ£ Data Preprocessing & Feature Engineering\n",
        "\n",
        "We need to convert text into tokens and encode labels as numerical values.\n",
        "\n",
        "ğŸ’¡ Explanation\n",
        "âœ” Maps sentiment labels â†’ Converts \"Positive\", \"Neutral\", \"Negative\" into numbers (2, 1, 0). âœ” Tokenizes text â†’ Transforms words into numerical format for model understanding. âœ” Pads sequences â†’ Standardizes input length for efficient training. âœ” Prepares features (X) & labels (y) â†’ Ensures data is ready for training."
      ],
      "metadata": {
        "id": "lG9jZTSqcFYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load dataset safely\n",
        "try:\n",
        "    df = pd.read_csv(\"twitter_training.csv\", encoding=\"utf-8\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File 'twitter_training.csv' not found. Ensure the file path is correct.\")\n",
        "    exit()\n",
        "\n",
        "# Display dataset structure\n",
        "print(\"\\nDataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\\n\", df.head())\n",
        "print(\"\\nColumn Names:\", df.columns)\n",
        "\n",
        "# Fix potential column name issues (removing extra spaces)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Identify correct sentiment column\n",
        "for col in df.columns:\n",
        "    print(f\"Unique values in column '{col}':\")\n",
        "    print(df[col].unique())  # Print unique values to find sentiment data\n",
        "\n",
        "# Rename the correct column containing sentiment values\n",
        "df.rename(columns={\"Positive\": \"sentiment\"}, inplace=True)  # Adjust based on actual data\n",
        "\n",
        "# Ensure sentiment column now exists\n",
        "if 'sentiment' not in df.columns:\n",
        "    print(\"\\nError: Correct sentiment column not found! Available Columns:\", df.columns)\n",
        "    exit()  # Stop execution if still missing\n",
        "\n",
        "# Handle missing sentiment values\n",
        "df = df.dropna(subset=[\"sentiment\"])  # Remove missing values\n",
        "\n",
        "# Verify unique sentiment values before encoding\n",
        "print(\"\\nSentiment values before encoding:\", df[\"sentiment\"].unique())\n",
        "\n",
        "# Encode sentiment labels (Adjust mapping as needed)\n",
        "sentiment_mapping = {\"Positive\": 2, \"Neutral\": 1, \"Negative\": 0}\n",
        "df[\"sentiment\"] = df[\"sentiment\"].map(sentiment_mapping)\n",
        "\n",
        "# Remove rows where sentiment encoding failed\n",
        "df = df.dropna(subset=[\"sentiment\"])  # Drop unmapped sentiment values\n",
        "\n",
        "# Recheck sentiment value counts after encoding\n",
        "print(\"\\nSentiment Value Counts:\\n\", df[\"sentiment\"].value_counts())\n",
        "\n",
        "# Identify the correct column containing tweet text (rename if necessary)\n",
        "text_column = \"im getting on borderlands and i will murder you all ,\"  # Adjust if needed\n",
        "\n",
        "# Ensure all text values are strings (fixing float issue)\n",
        "df[text_column] = df[text_column].astype(str)\n",
        "\n",
        "# Tokenize tweet text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[text_column])\n",
        "\n",
        "# Convert tweets to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df[text_column])\n",
        "\n",
        "# Pad sequences for uniform length\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "X = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Prepare labels (target variable)\n",
        "y = df[\"sentiment\"].values\n",
        "\n",
        "# Final check\n",
        "print(\"\\nX shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"\\nData preprocessing completed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P41wKJlPcOgO",
        "outputId": "50b0e7ab-ed12-4e6e-a2f7-0cc84adaa686"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Shape: (74681, 4)\n",
            "\n",
            "First few rows:\n",
            "    2401  Borderlands  Positive  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "  im getting on borderlands and i will murder you all ,  \n",
            "0  I am coming to the borders and I will kill you...     \n",
            "1  im getting on borderlands and i will kill you ...     \n",
            "2  im coming on borderlands and i will murder you...     \n",
            "3  im getting on borderlands 2 and i will murder ...     \n",
            "4  im getting into borderlands and i can murder y...     \n",
            "\n",
            "Column Names: Index(['2401', 'Borderlands', 'Positive',\n",
            "       'im getting on borderlands and i will murder you all ,'],\n",
            "      dtype='object')\n",
            "Unique values in column '2401':\n",
            "[2401 2402 2403 ... 9198 9199 9200]\n",
            "Unique values in column 'Borderlands':\n",
            "['Borderlands' 'CallOfDutyBlackopsColdWar' 'Amazon' 'Overwatch'\n",
            " 'Xbox(Xseries)' 'NBA2K' 'Dota2' 'PlayStation5(PS5)' 'WorldOfCraft'\n",
            " 'CS-GO' 'Google' 'AssassinsCreed' 'ApexLegends' 'LeagueOfLegends'\n",
            " 'Fortnite' 'Microsoft' 'Hearthstone' 'Battlefield'\n",
            " 'PlayerUnknownsBattlegrounds(PUBG)' 'Verizon' 'HomeDepot' 'FIFA'\n",
            " 'RedDeadRedemption(RDR)' 'CallOfDuty' 'TomClancysRainbowSix' 'Facebook'\n",
            " 'GrandTheftAuto(GTA)' 'MaddenNFL' 'johnson&johnson' 'Cyberpunk2077'\n",
            " 'TomClancysGhostRecon' 'Nvidia']\n",
            "Unique values in column 'Positive':\n",
            "['Positive' 'Neutral' 'Negative' 'Irrelevant']\n",
            "Unique values in column 'im getting on borderlands and i will murder you all ,':\n",
            "['I am coming to the borders and I will kill you all,'\n",
            " 'im getting on borderlands and i will kill you all,'\n",
            " 'im coming on borderlands and i will murder you all,' ...\n",
            " 'Just realized the windows partition of my Mac is now 6 years behind on Nvidia drivers and I have no idea how he didnâ€™t notice'\n",
            " 'Just realized between the windows partition of my Mac is like being 6 years behind on Nvidia drivers and cars I have no fucking idea how I ever didn â€™ t notice'\n",
            " 'Just like the windows partition of my Mac is like 6 years behind on its drivers So you have no idea how I didnâ€™t notice']\n",
            "\n",
            "Sentiment values before encoding: ['Positive' 'Neutral' 'Negative' 'Irrelevant']\n",
            "\n",
            "Sentiment Value Counts:\n",
            " sentiment\n",
            "0.0    22542\n",
            "2.0    20831\n",
            "1.0    18318\n",
            "Name: count, dtype: int64\n",
            "\n",
            "X shape: (61691, 166)\n",
            "y shape: (61691,)\n",
            "\n",
            "Data preprocessing completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4ï¸âƒ£ Split Dataset into Training & Testing Sets\n",
        "\n",
        "ğŸ’¡ Explanation\n",
        "âœ” Splits dataset (80% training, 20% testing) âœ” Ensures dataset is randomized for better learning"
      ],
      "metadata": {
        "id": "PbObtP4oeuie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "UmGGi22Le4Yn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5ï¸âƒ£ Define RNN Model Architecture\n",
        "\n",
        "We'll start with a simple RNN model.\n",
        "\n",
        "ğŸ’¡ Explanation\n",
        "âœ” Embedding layer â†’ Converts words into dense vector representations âœ” SimpleRNN layer â†’ Processes text sequences âœ” Dropout â†’ Helps prevent overfitting âœ” Dense output layer (softmax) â†’ Multi-class classification\n"
      ],
      "metadata": {
        "id": "H5j-zSRefDoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=max_length),\n",
        "    SimpleRNN(100, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation=\"softmax\")  # 3 classes (Positive, Neutral, Negative)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_rnn.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_rnn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "eLP9afJBfM9g",
        "outputId": "99fd1935-8137-4c08-860e-32408deed8ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6ï¸âƒ£ Train the RNN Model\n",
        "\n",
        "ğŸ’¡ Explanation\n",
        "âœ” Trains model for 10 epochs âœ” Uses batch size of 32 âœ” Validation split (20%) monitors generalization"
      ],
      "metadata": {
        "id": "THSLGrpLfRJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_rnn = model_rnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMIzkLtBfUiT",
        "outputId": "b8e68893-1d84-472e-c4fd-cecb1cab7809"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 131ms/step - accuracy: 0.3425 - loss: 1.1557 - val_accuracy: 0.3641 - val_loss: 1.0982\n",
            "Epoch 2/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 105ms/step - accuracy: 0.3419 - loss: 1.1094 - val_accuracy: 0.3646 - val_loss: 1.0948\n",
            "Epoch 3/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 106ms/step - accuracy: 0.3529 - loss: 1.1001 - val_accuracy: 0.3353 - val_loss: 1.1026\n",
            "Epoch 4/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 107ms/step - accuracy: 0.3564 - loss: 1.1000 - val_accuracy: 0.3637 - val_loss: 1.0941\n",
            "Epoch 5/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 108ms/step - accuracy: 0.3527 - loss: 1.0998 - val_accuracy: 0.3298 - val_loss: 1.1042\n",
            "Epoch 6/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 105ms/step - accuracy: 0.3535 - loss: 1.1021 - val_accuracy: 0.3750 - val_loss: 1.0913\n",
            "Epoch 7/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 107ms/step - accuracy: 0.3654 - loss: 1.0963 - val_accuracy: 0.3706 - val_loss: 1.0922\n",
            "Epoch 8/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 110ms/step - accuracy: 0.3588 - loss: 1.0987 - val_accuracy: 0.3567 - val_loss: 1.0945\n",
            "Epoch 9/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 105ms/step - accuracy: 0.3516 - loss: 1.0989 - val_accuracy: 0.3632 - val_loss: 1.0980\n",
            "Epoch 10/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 107ms/step - accuracy: 0.3499 - loss: 1.1005 - val_accuracy: 0.3571 - val_loss: 1.0926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7ï¸âƒ£ Evaluate the RNN Model\n",
        "\n",
        "ğŸ’¡ Explanation\n",
        "âœ” Converts model predictions to sentiment labels âœ” Evaluates accuracy, precision, recall & F1-score âœ” Balanced F1-score ensures strong classification performance"
      ],
      "metadata": {
        "id": "kEWVgxi3faKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_rnn.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes, average=\"weighted\")\n",
        "recall = recall_score(y_test, y_pred_classes, average=\"weighted\")\n",
        "f1 = f1_score(y_test, y_pred_classes, average=\"weighted\")\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlCFB5qqfd_I",
        "outputId": "1fcae5f8-0bb6-4a34-97ce-b2ec899cf8e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m386/386\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step\n",
            "Test Accuracy: 0.36\n",
            "Precision: 0.24\n",
            "Recall: 0.36\n",
            "F1 Score: 0.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8ï¸âƒ£ Hyperparameter Tuning & Optimization\n",
        "\n",
        "We can experiment with different architectures.\n",
        "\n",
        "ğŸ’¡ Explanation\n",
        "âœ” Uses LSTM for better memory retention âœ” Replaces SimpleRNN with LSTM âœ” Evaluates improvements in accuracy"
      ],
      "metadata": {
        "id": "EOtDcg6ZfglG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize using LSTM instead of SimpleRNN\n",
        "model_lstm = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=max_length),\n",
        "    LSTM(100, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_lstm.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history_lstm = model_lstm.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss_lstm, accuracy_lstm = model_lstm.evaluate(X_test, y_test)\n",
        "print(f\"LSTM Model Accuracy: {accuracy_lstm:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMmXyXmFfoJx",
        "outputId": "dadbfa1e-f3a4-4bac-aad9-a5c599891f3f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 243ms/step - accuracy: 0.3522 - loss: 1.0972 - val_accuracy: 0.3706 - val_loss: 1.0951\n",
            "Epoch 2/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 253ms/step - accuracy: 0.3631 - loss: 1.0959 - val_accuracy: 0.3706 - val_loss: 1.0950\n",
            "Epoch 3/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 265ms/step - accuracy: 0.3607 - loss: 1.0956 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 4/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 249ms/step - accuracy: 0.3647 - loss: 1.0949 - val_accuracy: 0.3706 - val_loss: 1.0952\n",
            "Epoch 5/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 249ms/step - accuracy: 0.3646 - loss: 1.0954 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 6/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 249ms/step - accuracy: 0.3656 - loss: 1.0948 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 7/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 248ms/step - accuracy: 0.3665 - loss: 1.0953 - val_accuracy: 0.3706 - val_loss: 1.0945\n",
            "Epoch 8/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 243ms/step - accuracy: 0.3617 - loss: 1.0957 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 9/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 247ms/step - accuracy: 0.3662 - loss: 1.0950 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 10/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 243ms/step - accuracy: 0.3627 - loss: 1.0958 - val_accuracy: 0.3706 - val_loss: 1.0946\n",
            "\u001b[1m386/386\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 71ms/step - accuracy: 0.3707 - loss: 1.0945\n",
            "LSTM Model Accuracy: 0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9ï¸âƒ£ Predict Sentiment for New Tweets\n",
        "\n",
        "Let's predict sentiment for an input tweet.\n",
        "\n",
        "ğŸ’¡ Explanation\n",
        "âœ” Takes a new tweet and predicts its sentiment âœ” Uses trained LSTM model for classification âœ” Maps numeric predictions back to sentiment labels"
      ],
      "metadata": {
        "id": "d9oJcVrmfqmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, tokenizer, text, max_length):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = pad_sequences(sequence, maxlen=max_length, padding=\"post\")\n",
        "    predicted_index = np.argmax(model.predict(sequence))\n",
        "    sentiment_label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    return sentiment_label[predicted_index]\n",
        "\n",
        "input_text = \"I love this product!\"\n",
        "print(\"Predicted Sentiment:\", predict_sentiment(model_lstm, tokenizer, input_text, max_length))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rug-ZSjfxLQ",
        "outputId": "b652a875-e02c-40af-f06c-96199519da03"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step\n",
            "Predicted Sentiment: Negative\n"
          ]
        }
      ]
    }
  ]
}