{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAqZFfUBSYns3JdGqR1r/E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishisg/ChatGPT/blob/main/RNN_Sentiment_Analysis_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå RNN for Sentiment Analysis\n",
        "\n",
        "‚úÖ Dataset: Twitter Sentiment Analysis ‚úÖ Goal: Classify tweets as Positive, Negative, or Neutral ‚úÖ Approach: Build RNN-based deep learning models using TensorFlow/Keras ‚úÖ Evaluation: Accuracy, Precision, Recall, F1-score, Model Optimization\n",
        "\n",
        "1Ô∏è‚É£ Setup & Import Libraries\n",
        "\n",
        "üí° Explanation\n",
        "‚úî numpy & pandas ‚Üí Handle numerical & text data ‚úî tensorflow & keras ‚Üí Build RNN models ‚úî Tokenizer & pad_sequences ‚Üí Convert text to numerical format for deep learning ‚úî SimpleRNN, LSTM, GRU ‚Üí RNN architectures for sentiment analysis"
      ],
      "metadata": {
        "id": "yUWHGMVyajkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WGqrh5Edainx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Load & Explore the Dataset\n",
        "\n",
        "üí° Explanation\n",
        "‚úî Loads Twitter Sentiment Analysis dataset ‚úî Shows sample tweets & sentiment labels ‚úî Class distribution check ensures a balanced dataset\n",
        "\n",
        "‚úî Loads the Twitter Training CSV file with proper encoding ‚úî Checks if the dataset structure is valid ‚úî Prints available columns & verifies if 'sentiment' exists ‚úî Strips column names to remove extra spaces ‚úî Provides manual header renaming options if needed\n"
      ],
      "metadata": {
        "id": "LEdCBH0la2EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary library\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (Ensuring correct encoding)\n",
        "df = pd.read_csv(\"twitter_training.csv\", encoding=\"utf-8\")\n",
        "\n",
        "# Check dataset structure\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"First few rows:\\n\", df.head())\n",
        "print(\"Column Names:\", df.columns)\n",
        "\n",
        "# Fix potential column name issues (removing extra spaces)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Verify if 'sentiment' column exists\n",
        "if 'sentiment' in df.columns:\n",
        "    print(\"Sentiment Column Found!\")\n",
        "    print(df[\"sentiment\"].value_counts())  # Check class distribution\n",
        "else:\n",
        "    print(\"Error: Column 'sentiment' not found!\")\n",
        "    print(\"Available Columns:\", df.columns)\n",
        "\n",
        "# Optional Fix: If headers are incorrect, rename columns manually\n",
        "# Uncomment and update column names based on actual structure\n",
        "# df.columns = [\"id\", \"user\", \"tweet\", \"sentiment\"]\n",
        "# print(\"Updated Columns:\", df.columns)\n",
        "\n",
        "# Optional Fix: Reload without header row if structure looks incorrect\n",
        "# df = pd.read_csv(\"twitter_training.csv\", header=None)\n",
        "# print(\"Head after loading without header:\\n\", df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sttOGUyYa8MT",
        "outputId": "796c0e6d-406a-4326-9455-1bfa77836f8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (74681, 4)\n",
            "First few rows:\n",
            "    2401  Borderlands  Positive  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "  im getting on borderlands and i will murder you all ,  \n",
            "0  I am coming to the borders and I will kill you...     \n",
            "1  im getting on borderlands and i will kill you ...     \n",
            "2  im coming on borderlands and i will murder you...     \n",
            "3  im getting on borderlands 2 and i will murder ...     \n",
            "4  im getting into borderlands and i can murder y...     \n",
            "Column Names: Index(['2401', 'Borderlands', 'Positive',\n",
            "       'im getting on borderlands and i will murder you all ,'],\n",
            "      dtype='object')\n",
            "Error: Column 'sentiment' not found!\n",
            "Available Columns: Index(['2401', 'Borderlands', 'Positive',\n",
            "       'im getting on borderlands and i will murder you all ,'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Data Preprocessing & Feature Engineering\n",
        "\n",
        "We need to convert text into tokens and encode labels as numerical values.\n",
        "\n",
        "üí° Explanation\n",
        "‚úî Maps sentiment labels ‚Üí Converts \"Positive\", \"Neutral\", \"Negative\" into numbers (2, 1, 0). ‚úî Tokenizes text ‚Üí Transforms words into numerical format for model understanding. ‚úî Pads sequences ‚Üí Standardizes input length for efficient training. ‚úî Prepares features (X) & labels (y) ‚Üí Ensures data is ready for training."
      ],
      "metadata": {
        "id": "lG9jZTSqcFYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load dataset safely\n",
        "try:\n",
        "    df = pd.read_csv(\"twitter_training.csv\", encoding=\"utf-8\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File 'twitter_training.csv' not found. Ensure the file path is correct.\")\n",
        "    exit()\n",
        "\n",
        "# Display dataset structure\n",
        "print(\"\\nDataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\\n\", df.head())\n",
        "print(\"\\nColumn Names:\", df.columns)\n",
        "\n",
        "# Fix potential column name issues (removing extra spaces)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Identify correct sentiment column\n",
        "for col in df.columns:\n",
        "    print(f\"Unique values in column '{col}':\")\n",
        "    print(df[col].unique())  # Print unique values to find sentiment data\n",
        "\n",
        "# Rename the correct column containing sentiment values\n",
        "df.rename(columns={\"Positive\": \"sentiment\"}, inplace=True)  # Adjust based on actual data\n",
        "\n",
        "# Ensure sentiment column now exists\n",
        "if 'sentiment' not in df.columns:\n",
        "    print(\"\\nError: Correct sentiment column not found! Available Columns:\", df.columns)\n",
        "    exit()  # Stop execution if still missing\n",
        "\n",
        "# Handle missing sentiment values\n",
        "df = df.dropna(subset=[\"sentiment\"])  # Remove missing values\n",
        "\n",
        "# Verify unique sentiment values before encoding\n",
        "print(\"\\nSentiment values before encoding:\", df[\"sentiment\"].unique())\n",
        "\n",
        "# Encode sentiment labels (Adjust mapping as needed)\n",
        "sentiment_mapping = {\"Positive\": 2, \"Neutral\": 1, \"Negative\": 0}\n",
        "df[\"sentiment\"] = df[\"sentiment\"].map(sentiment_mapping)\n",
        "\n",
        "# Remove rows where sentiment encoding failed\n",
        "df = df.dropna(subset=[\"sentiment\"])  # Drop unmapped sentiment values\n",
        "\n",
        "# Recheck sentiment value counts after encoding\n",
        "print(\"\\nSentiment Value Counts:\\n\", df[\"sentiment\"].value_counts())\n",
        "\n",
        "# Identify the correct column containing tweet text (rename if necessary)\n",
        "text_column = \"im getting on borderlands and i will murder you all ,\"  # Adjust if needed\n",
        "\n",
        "# Ensure all text values are strings (fixing float issue)\n",
        "df[text_column] = df[text_column].astype(str)\n",
        "\n",
        "# Tokenize tweet text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[text_column])\n",
        "\n",
        "# Convert tweets to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df[text_column])\n",
        "\n",
        "# Pad sequences for uniform length\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "X = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Prepare labels (target variable)\n",
        "y = df[\"sentiment\"].values\n",
        "\n",
        "# Final check\n",
        "print(\"\\nX shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"\\nData preprocessing completed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P41wKJlPcOgO",
        "outputId": "50b0e7ab-ed12-4e6e-a2f7-0cc84adaa686"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Shape: (74681, 4)\n",
            "\n",
            "First few rows:\n",
            "    2401  Borderlands  Positive  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "  im getting on borderlands and i will murder you all ,  \n",
            "0  I am coming to the borders and I will kill you...     \n",
            "1  im getting on borderlands and i will kill you ...     \n",
            "2  im coming on borderlands and i will murder you...     \n",
            "3  im getting on borderlands 2 and i will murder ...     \n",
            "4  im getting into borderlands and i can murder y...     \n",
            "\n",
            "Column Names: Index(['2401', 'Borderlands', 'Positive',\n",
            "       'im getting on borderlands and i will murder you all ,'],\n",
            "      dtype='object')\n",
            "Unique values in column '2401':\n",
            "[2401 2402 2403 ... 9198 9199 9200]\n",
            "Unique values in column 'Borderlands':\n",
            "['Borderlands' 'CallOfDutyBlackopsColdWar' 'Amazon' 'Overwatch'\n",
            " 'Xbox(Xseries)' 'NBA2K' 'Dota2' 'PlayStation5(PS5)' 'WorldOfCraft'\n",
            " 'CS-GO' 'Google' 'AssassinsCreed' 'ApexLegends' 'LeagueOfLegends'\n",
            " 'Fortnite' 'Microsoft' 'Hearthstone' 'Battlefield'\n",
            " 'PlayerUnknownsBattlegrounds(PUBG)' 'Verizon' 'HomeDepot' 'FIFA'\n",
            " 'RedDeadRedemption(RDR)' 'CallOfDuty' 'TomClancysRainbowSix' 'Facebook'\n",
            " 'GrandTheftAuto(GTA)' 'MaddenNFL' 'johnson&johnson' 'Cyberpunk2077'\n",
            " 'TomClancysGhostRecon' 'Nvidia']\n",
            "Unique values in column 'Positive':\n",
            "['Positive' 'Neutral' 'Negative' 'Irrelevant']\n",
            "Unique values in column 'im getting on borderlands and i will murder you all ,':\n",
            "['I am coming to the borders and I will kill you all,'\n",
            " 'im getting on borderlands and i will kill you all,'\n",
            " 'im coming on borderlands and i will murder you all,' ...\n",
            " 'Just realized the windows partition of my Mac is now 6 years behind on Nvidia drivers and I have no idea how he didn‚Äôt notice'\n",
            " 'Just realized between the windows partition of my Mac is like being 6 years behind on Nvidia drivers and cars I have no fucking idea how I ever didn ‚Äô t notice'\n",
            " 'Just like the windows partition of my Mac is like 6 years behind on its drivers So you have no idea how I didn‚Äôt notice']\n",
            "\n",
            "Sentiment values before encoding: ['Positive' 'Neutral' 'Negative' 'Irrelevant']\n",
            "\n",
            "Sentiment Value Counts:\n",
            " sentiment\n",
            "0.0    22542\n",
            "2.0    20831\n",
            "1.0    18318\n",
            "Name: count, dtype: int64\n",
            "\n",
            "X shape: (61691, 166)\n",
            "y shape: (61691,)\n",
            "\n",
            "Data preprocessing completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4Ô∏è‚É£ Split Dataset into Training & Testing Sets\n",
        "\n",
        "üí° Explanation\n",
        "‚úî Splits dataset (80% training, 20% testing) ‚úî Ensures dataset is randomized for better learning"
      ],
      "metadata": {
        "id": "PbObtP4oeuie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "UmGGi22Le4Yn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5Ô∏è‚É£ Define RNN Model Architecture\n",
        "\n",
        "We'll start with a simple RNN model.\n",
        "\n",
        "üí° Explanation\n",
        "‚úî Embedding layer ‚Üí Converts words into dense vector representations ‚úî SimpleRNN layer ‚Üí Processes text sequences ‚úî Dropout ‚Üí Helps prevent overfitting ‚úî Dense output layer (softmax) ‚Üí Multi-class classification\n"
      ],
      "metadata": {
        "id": "H5j-zSRefDoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=max_length),\n",
        "    SimpleRNN(100, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation=\"softmax\")  # 3 classes (Positive, Neutral, Negative)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_rnn.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_rnn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "eLP9afJBfM9g",
        "outputId": "99fd1935-8137-4c08-860e-32408deed8ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ ?                      ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ ?                      ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6Ô∏è‚É£ Train the RNN Model\n",
        "\n",
        "üí° Explanation\n",
        "‚úî Trains model for 10 epochs ‚úî Uses batch size of 32 ‚úî Validation split (20%) monitors generalization"
      ],
      "metadata": {
        "id": "THSLGrpLfRJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_rnn = model_rnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMIzkLtBfUiT",
        "outputId": "b8e68893-1d84-472e-c4fd-cecb1cab7809"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 131ms/step - accuracy: 0.3425 - loss: 1.1557 - val_accuracy: 0.3641 - val_loss: 1.0982\n",
            "Epoch 2/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 105ms/step - accuracy: 0.3419 - loss: 1.1094 - val_accuracy: 0.3646 - val_loss: 1.0948\n",
            "Epoch 3/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 106ms/step - accuracy: 0.3529 - loss: 1.1001 - val_accuracy: 0.3353 - val_loss: 1.1026\n",
            "Epoch 4/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 107ms/step - accuracy: 0.3564 - loss: 1.1000 - val_accuracy: 0.3637 - val_loss: 1.0941\n",
            "Epoch 5/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 108ms/step - accuracy: 0.3527 - loss: 1.0998 - val_accuracy: 0.3298 - val_loss: 1.1042\n",
            "Epoch 6/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 105ms/step - accuracy: 0.3535 - loss: 1.1021 - val_accuracy: 0.3750 - val_loss: 1.0913\n",
            "Epoch 7/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 107ms/step - accuracy: 0.3654 - loss: 1.0963 - val_accuracy: 0.3706 - val_loss: 1.0922\n",
            "Epoch 8/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 110ms/step - accuracy: 0.3588 - loss: 1.0987 - val_accuracy: 0.3567 - val_loss: 1.0945\n",
            "Epoch 9/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 105ms/step - accuracy: 0.3516 - loss: 1.0989 - val_accuracy: 0.3632 - val_loss: 1.0980\n",
            "Epoch 10/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 107ms/step - accuracy: 0.3499 - loss: 1.1005 - val_accuracy: 0.3571 - val_loss: 1.0926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7Ô∏è‚É£ Evaluate the RNN Model\n",
        "\n",
        "üí° Explanation\n",
        "‚úî Converts model predictions to sentiment labels ‚úî Evaluates accuracy, precision, recall & F1-score ‚úî Balanced F1-score ensures strong classification performance"
      ],
      "metadata": {
        "id": "kEWVgxi3faKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_rnn.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes, average=\"weighted\")\n",
        "recall = recall_score(y_test, y_pred_classes, average=\"weighted\")\n",
        "f1 = f1_score(y_test, y_pred_classes, average=\"weighted\")\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlCFB5qqfd_I",
        "outputId": "1fcae5f8-0bb6-4a34-97ce-b2ec899cf8e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m386/386\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step\n",
            "Test Accuracy: 0.36\n",
            "Precision: 0.24\n",
            "Recall: 0.36\n",
            "F1 Score: 0.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8Ô∏è‚É£ Hyperparameter Tuning & Optimization\n",
        "\n",
        "We can experiment with different architectures.\n",
        "\n",
        "üí° Explanation\n",
        "‚úî Uses LSTM for better memory retention ‚úî Replaces SimpleRNN with LSTM ‚úî Evaluates improvements in accuracy"
      ],
      "metadata": {
        "id": "EOtDcg6ZfglG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize using LSTM instead of SimpleRNN\n",
        "model_lstm = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=max_length),\n",
        "    LSTM(100, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_lstm.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history_lstm = model_lstm.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss_lstm, accuracy_lstm = model_lstm.evaluate(X_test, y_test)\n",
        "print(f\"LSTM Model Accuracy: {accuracy_lstm:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMmXyXmFfoJx",
        "outputId": "dadbfa1e-f3a4-4bac-aad9-a5c599891f3f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 243ms/step - accuracy: 0.3522 - loss: 1.0972 - val_accuracy: 0.3706 - val_loss: 1.0951\n",
            "Epoch 2/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 253ms/step - accuracy: 0.3631 - loss: 1.0959 - val_accuracy: 0.3706 - val_loss: 1.0950\n",
            "Epoch 3/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 265ms/step - accuracy: 0.3607 - loss: 1.0956 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 4/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 249ms/step - accuracy: 0.3647 - loss: 1.0949 - val_accuracy: 0.3706 - val_loss: 1.0952\n",
            "Epoch 5/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 249ms/step - accuracy: 0.3646 - loss: 1.0954 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 6/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 249ms/step - accuracy: 0.3656 - loss: 1.0948 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 7/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 248ms/step - accuracy: 0.3665 - loss: 1.0953 - val_accuracy: 0.3706 - val_loss: 1.0945\n",
            "Epoch 8/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 243ms/step - accuracy: 0.3617 - loss: 1.0957 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 9/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 247ms/step - accuracy: 0.3662 - loss: 1.0950 - val_accuracy: 0.3706 - val_loss: 1.0944\n",
            "Epoch 10/10\n",
            "\u001b[1m1234/1234\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 243ms/step - accuracy: 0.3627 - loss: 1.0958 - val_accuracy: 0.3706 - val_loss: 1.0946\n",
            "\u001b[1m386/386\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 71ms/step - accuracy: 0.3707 - loss: 1.0945\n",
            "LSTM Model Accuracy: 0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9Ô∏è‚É£ Predict Sentiment for New Tweets\n",
        "\n",
        "Let's predict sentiment for an input tweet.\n",
        "\n",
        "üí° Explanation\n",
        "‚úî Takes a new tweet and predicts its sentiment ‚úî Uses trained LSTM model for classification ‚úî Maps numeric predictions back to sentiment labels"
      ],
      "metadata": {
        "id": "d9oJcVrmfqmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, tokenizer, text, max_length):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = pad_sequences(sequence, maxlen=max_length, padding=\"post\")\n",
        "    predicted_index = np.argmax(model.predict(sequence))\n",
        "    sentiment_label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    return sentiment_label[predicted_index]\n",
        "\n",
        "input_text = \"I love this product!\"\n",
        "print(\"Predicted Sentiment:\", predict_sentiment(model_lstm, tokenizer, input_text, max_length))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rug-ZSjfxLQ",
        "outputId": "b652a875-e02c-40af-f06c-96199519da03"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step\n",
            "Predicted Sentiment: Negative\n"
          ]
        }
      ]
    }
  ]
}