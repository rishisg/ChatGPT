{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPM0Goh0x+bmdqyP8ugMQW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishisg/ChatGPT/blob/main/LSTM_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xja8FQwgle0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53e98ce4-6b53-49a8-ec68-b3cebe85b961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Setup and Data Preparation\n",
            "\n",
            "Dataset loaded successfully.\n",
            "First 200 characters of the dataset:\n",
            "﻿The Project Gutenberg eBook of Pride and Prejudice\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "\n",
            "\n",
            "Total unique words in the dataset: 7561\n",
            "Tokenization completed.\n",
            "\n",
            "Max sequence length: 25\n",
            "Input sequences generated and padded.\n",
            "\n",
            "Shape of X: (121111, 24)\n",
            "Shape of y: (121111, 7561)\n",
            "Data preprocessing completed.\n",
            "\n",
            "Step 2: LSTM Model Architecture\n",
            "\n",
            "Model architecture summary:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Building the LSTM Model\n",
            "\n",
            "Model compiled successfully.\n",
            "\n",
            "Step 4: Model Training\n",
            "Epoch 1/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 235ms/step - accuracy: 0.0343 - loss: 6.7754 - val_accuracy: 0.0496 - val_loss: 6.3628\n",
            "Epoch 2/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 224ms/step - accuracy: 0.0583 - loss: 6.0743 - val_accuracy: 0.0644 - val_loss: 6.1716\n",
            "Epoch 3/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 235ms/step - accuracy: 0.0812 - loss: 5.7909 - val_accuracy: 0.0883 - val_loss: 6.0591\n",
            "Epoch 4/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 237ms/step - accuracy: 0.0998 - loss: 5.6061 - val_accuracy: 0.0924 - val_loss: 6.0043\n",
            "Epoch 5/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 235ms/step - accuracy: 0.1131 - loss: 5.4490 - val_accuracy: 0.1074 - val_loss: 5.9547\n",
            "Epoch 6/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 234ms/step - accuracy: 0.1216 - loss: 5.3355 - val_accuracy: 0.1103 - val_loss: 5.9182\n",
            "Epoch 7/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 234ms/step - accuracy: 0.1291 - loss: 5.2169 - val_accuracy: 0.1126 - val_loss: 5.8928\n",
            "Epoch 8/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 235ms/step - accuracy: 0.1351 - loss: 5.1176 - val_accuracy: 0.1175 - val_loss: 5.8851\n",
            "Epoch 9/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 228ms/step - accuracy: 0.1401 - loss: 5.0367 - val_accuracy: 0.1167 - val_loss: 5.8955\n",
            "Epoch 10/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 227ms/step - accuracy: 0.1469 - loss: 4.9433 - val_accuracy: 0.1189 - val_loss: 5.9061\n",
            "Epoch 11/50\n",
            "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 228ms/step - accuracy: 0.1489 - loss: 4.8726 - val_accuracy: 0.1210 - val_loss: 5.9326\n",
            "\n",
            "Model training completed.\n",
            "\n",
            "Step 5: Model Evaluation\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Step 1: Setup and Data Preparation\")\n",
        "# Load the dataset\n",
        "with open('LSTM DATA.txt', 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "print(\"\\nDataset loaded successfully.\")\n",
        "print(f\"First 200 characters of the dataset:\\n{data[:200]}\")\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "total_words = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
        "\n",
        "print(f\"\\nTotal unique words in the dataset: {total_words}\")\n",
        "print(\"Tokenization completed.\")\n",
        "\n",
        "# Generate input sequences\n",
        "input_sequences = []\n",
        "for line in data.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "print(f\"\\nMax sequence length: {max_sequence_len}\")\n",
        "print(\"Input sequences generated and padded.\")\n",
        "\n",
        "# Split into predictors (X) and labels (y)\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# Convert labels to categorical format\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "print(f\"\\nShape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "print(\"Data preprocessing completed.\")\n",
        "\n",
        "print(\"\\nStep 2: LSTM Model Architecture\")\n",
        "# Define the LSTM model architecture\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_words, output_dim=100, input_length=max_sequence_len-1),\n",
        "    LSTM(150, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(100),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "print(\"\\nModel architecture summary:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nStep 3: Building the LSTM Model\")\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nModel compiled successfully.\")\n",
        "\n",
        "print(\"\\nStep 4: Model Training\")\n",
        "# Define early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X, y,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "print(\"\\nModel training completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Define the Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=64, input_length=100),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification example\n",
        "])\n",
        "\n",
        "# Step 2: Compile the Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Prepare Training Data\n",
        "X_train = np.random.randint(0, 1000, size=(1000, 100))  # Random integer sequences\n",
        "y_train = np.random.randint(0, 2, size=(1000,))        # Binary labels\n",
        "\n",
        "# Train the Model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Step 4: Prepare Test Data\n",
        "X_test = np.random.randint(0, 1000, size=(200, 100))  # Random integer sequences\n",
        "y_test = np.random.randint(0, 2, size=(200,))        # Binary labels\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "print(\"\\nStep 5: Model Evaluation\")\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Loss: {loss:.2f}, Test Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUVEHEGg0IGh",
        "outputId": "b785a6a2-2779-4c08-8e55-e423fb325b34"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.5377 - loss: 0.6920 - val_accuracy: 0.4800 - val_loss: 0.6945\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9879 - loss: 0.5657 - val_accuracy: 0.4800 - val_loss: 0.7054\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.3273 - val_accuracy: 0.4850 - val_loss: 0.7455\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0573 - val_accuracy: 0.4250 - val_loss: 0.8173\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0120 - val_accuracy: 0.4400 - val_loss: 0.8458\n",
            "\n",
            "Step 5: Model Evaluation\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5096 - loss: 0.8053 \n",
            "\n",
            "Test Loss: 0.78, Test Accuracy: 0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Step 1: Load and Preprocess Data\n",
        "texts = [\n",
        "    \"im getting on borderlands and i will murder you all\",\n",
        "    \"So I spent a few hours making something for fun\",\n",
        "    \"Rock-Hard La Varlope, RARE & POWERFUL\",\n",
        "    \"that was the first borderlands session in a long time where i actually had a really satisfying combat experience\"\n",
        "]\n",
        "labels = [1, 1, 0, 1]  # Example binary labels (1 = Positive, 0 = Neutral/Negative)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=100)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Define the Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Fine-tune the Model\n",
        "print(\"\\nStep 6: Fine-tuning and Optimization\")\n",
        "print(\"Fine-tuning the model...\")\n",
        "\n",
        "# Adjust the learning rate\n",
        "model.optimizer.learning_rate = 0.001\n",
        "\n",
        "# Fine-tune the model on the training data\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Step 4: Evaluate the Fine-tuned Model\n",
        "print(\"\\nEvaluating the fine-tuned model...\")\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.2f}, Validation Accuracy: {val_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNYh5jeo1nri",
        "outputId": "353e23e2-fc9c-4b07-9dad-385fc6681566"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6: Fine-tuning and Optimization\n",
            "Fine-tuning the model...\n",
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.6667 - loss: 0.6893 - val_accuracy: 0.0000e+00 - val_loss: 0.7009\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 635ms/step - accuracy: 0.3333 - loss: 0.6925 - val_accuracy: 0.0000e+00 - val_loss: 0.7015\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6667 - loss: 0.6836 - val_accuracy: 0.0000e+00 - val_loss: 0.7017\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.6824 - val_accuracy: 0.0000e+00 - val_loss: 0.7007\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 0.6764 - val_accuracy: 0.0000e+00 - val_loss: 0.6986\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.6577 - val_accuracy: 0.0000e+00 - val_loss: 0.6961\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 1.0000 - loss: 0.6481 - val_accuracy: 0.0000e+00 - val_loss: 0.6934\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 0.6462 - val_accuracy: 1.0000 - val_loss: 0.6886\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 1.0000 - loss: 0.6297 - val_accuracy: 1.0000 - val_loss: 0.6838\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.6501 - val_accuracy: 1.0000 - val_loss: 0.6769\n",
            "\n",
            "Evaluating the fine-tuned model...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.6769\n",
            "Validation Loss: 0.68, Validation Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\nStep 7: Next Word Prediction Function\")\n",
        "# Function to predict the next word\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "texts = [\n",
        "    \"im getting on borderlands and i will murder you all\",\n",
        "    \"So I spent a few hours making something for fun\",\n",
        "    \"Rock-Hard La Varlope, RARE & POWERFUL\",\n",
        "    \"that was the first borderlands session in a long time where i actually had a really satisfying combat experience\"\n",
        "]\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer(num_words=5000)  # Limit vocabulary to 5000 words\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Determine max_sequence_len (maximum length of sequences in the dataset)\n",
        "max_sequence_len = max(len(seq.split()) for seq in texts)\n",
        "\n",
        "# Example model (replace this with your trained model)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=max_sequence_len - 1),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dense(5000, activation='softmax')  # Output layer for vocabulary size\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 2: Define the prediction function\n",
        "def predict_next_word(seed_text, num_words):\n",
        "    \"\"\"\n",
        "    Predict the next words given a seed text.\n",
        "\n",
        "    Args:\n",
        "        seed_text (str): The input text to generate predictions from.\n",
        "        num_words (int): Number of words to predict.\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted sentence.\n",
        "    \"\"\"\n",
        "    for _ in range(num_words):\n",
        "        # Convert the seed text into a sequence of tokens\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "        # Pad the sequence to match the model's input length\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "\n",
        "        # Predict the probabilities for the next word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted_index = np.argmax(predicted_probs, axis=1)[0]\n",
        "\n",
        "        # Map the predicted index back to a word\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the seed text\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Step 3: Test the prediction function\n",
        "test_sentence = \"I love\"\n",
        "predicted_sentence = predict_next_word(test_sentence, num_words=3)\n",
        "print(f\"\\nPredicted sentence for '{test_sentence}': {predicted_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRUjHWt03BkZ",
        "outputId": "bfe03a65-348e-4323-8c6d-0c114c96ab03"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7: Next Word Prediction Function\n",
            "\n",
            "Predicted sentence for 'I love': I love   \n"
          ]
        }
      ]
    }
  ]
}